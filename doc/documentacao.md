
# Documenta√ß√£o do Projeto: Pipeline de Processamento de Dados ENA (Energia Natural Afluente)

# üìä Projeto: Sauter University 2025 Challenge
**Autor:** *Gen√°rio Correia de Azevedo*  
**Fun√ß√£o:** Engenharia de Dados  

## 1. Introdu√ß√£o

### 1.1. Vis√£o Geral do Projeto
Este documento detalha o projeto de pipeline de dados para o processamento de informa√ß√µes sobre Energia Natural Afluente (ENA), desenvolvido como parte do **Sauter University 2025 Challenge**. 

## 2. Arquitetura Geral

### 2.1. Componentes Principais
A arquitetura do pipeline de dados ENA √© composta por tr√™s componentes principais do Google Cloud Platform, que trabalham em conjunto para garantir o fluxo cont√≠nuo e eficiente dos dados:

*   **Google Cloud Storage (GCS)**: Atua como o data lake para o armazenamento de arquivos raw em formato Parquet. Os dados s√£o organizados em uma estrutura particionada por data (e.g., `gs://affable-elf-472819-k2-raw-data/dt=YYYY-MM-DD`), facilitando a gest√£o e o acesso otimizado.
*   **BigQuery**: Serve como o data warehouse anal√≠tico, hospedando tabelas externas que referenciam os dados no GCS, tabelas trusted com dados processados e limpos, e procedimentos armazenados para a execu√ß√£o de transforma√ß√µes complexas. Sua natureza serverless e escal√°vel o torna ideal para consultas SQL em grandes volumes de dados.
*   **Cloud Functions**: Uma fun√ß√£o serverless desenvolvida em Python, configurada para ser acionada via HTTP trigger. Esta fun√ß√£o √© respons√°vel por orquestrar a execu√ß√£o do procedimento armazenado no BigQuery, que realiza as opera√ß√µes de transforma√ß√£o e carga dos dados da camada raw para a trusted. Pode ser integrada com o Cloud Scheduler para automa√ß√£o di√°ria do processo.

### 2.2. Fluxo de Dados
O fluxo de dados no pipeline ENA segue uma sequ√™ncia l√≥gica, garantindo que os dados sejam processados de forma incremental e confi√°vel:

1.  **Ingest√£o de Dados Raw**: Os arquivos de dados brutos de ENA, tipicamente em formato Parquet, s√£o carregados no bucket do Google Cloud Storage designado para dados raw (`gs://affable-elf-472819-k2-raw-data/`). A organiza√ß√£o desses arquivos segue um padr√£o de particionamento por data.
2.  **Exposi√ß√£o no BigQuery**: Uma tabela externa no BigQuery (`ena_raw.tabela_externa_raw_data`) √© criada para apontar diretamente para o diret√≥rio no GCS onde os dados raw est√£o armazenados. Isso permite que o BigQuery consulte esses dados sem a necessidade de import√°-los fisicamente, mantendo a flexibilidade e reduzindo custos de armazenamento.
3.  **Orquestra√ß√£o e Transforma√ß√£o**: Uma Cloud Function √© acionada (manualmente via HTTP request ou automaticamente via Cloud Scheduler). Esta fun√ß√£o executa um procedimento armazenado no BigQuery (`ena_trusted.sp_atualizar_ena_trusted`). Este procedimento √© respons√°vel por ler os dados da tabela externa raw, aplicar as transforma√ß√µes necess√°rias (limpeza, tipagem, padroniza√ß√£o) e realizar uma opera√ß√£o de upsert (MERGE) na tabela trusted (`ena_trusted.tabela_ena_trusted`).
4.  **An√°lise e Valida√ß√£o**: Ap√≥s o processamento, queries de an√°lise preliminar s√£o executadas na tabela trusted para validar a qualidade e a integridade dos dados, garantindo que as transforma√ß√µes foram aplicadas corretamente e que os dados est√£o prontos para consumo por aplica√ß√µes anal√≠ticas ou modelos de IA.

### 2.3. Benef√≠cios da Arquitetura
A arquitetura proposta para o pipeline de dados ENA oferece uma s√©rie de benef√≠cios significativos, que contribuem para a efici√™ncia, escalabilidade e sustentabilidade do projeto:

*   **Escalabilidade**: O BigQuery √© projetado para processar consultas em paralelo sobre grandes volumes de dados, escalando automaticamente conforme a demanda. Da mesma forma, o Cloud Functions escala de forma el√°stica para lidar com picos de execu√ß√£o, sem interven√ß√£o manual.
*   **Custo-Efetividade**: O modelo de precifica√ß√£o do GCP, baseado em pagamento por uso, √© altamente vantajoso. No BigQuery, os custos s√£o principalmente associados √† quantidade de dados escaneados por consulta e ao armazenamento. No Cloud Functions, o custo √© determinado pelo n√∫mero de invoca√ß√µes e pelo tempo de execu√ß√£o, tornando a solu√ß√£o econ√¥mica para cargas de trabalho intermitentes ou agendadas.
*   **Seguran√ßa**: O GCP oferece recursos de seguran√ßa robustos. O acesso aos recursos √© controlado por meio de IAM (Identity and Access Management) roles, garantindo que apenas usu√°rios e servi√ßos autorizados possam interagir com os dados e as fun√ß√µes. Al√©m disso, os dados armazenados no GCS e BigQuery s√£o criptografados por padr√£o, tanto em tr√¢nsito quanto em repouso.
*   **Integra√ß√£o Nativa**: A utiliza√ß√£o de ferramentas nativas do GCP (GCS, BigQuery, Cloud Functions) simplifica a integra√ß√£o entre os componentes, reduzindo a complexidade de desenvolvimento e o overhead de manuten√ß√£o. Isso permite que a equipe se concentre na l√≥gica de neg√≥cio e na qualidade dos dados, em vez de gerenciar a infraestrutura subjacente.
*   **Resili√™ncia e Alta Disponibilidade**: Os servi√ßos do GCP s√£o projetados para alta disponibilidade e resili√™ncia, com redund√¢ncia em v√°rias regi√µes e zonas de disponibilidade, minimizando o risco de interrup√ß√µes no pipeline de dados.

## 3. Detalhamento dos Componentes

### 3.1. Tabela Externa Raw (`CREATEEXTERNALENA_RAW.sql`)

Este script SQL √© respons√°vel pela cria√ß√£o ou substitui√ß√£o de uma tabela externa no BigQuery que referencia os dados brutos de ENA armazenados no Google Cloud Storage (GCS). A tabela externa, denominada `ena_raw.tabela_externa_raw_data`, atua como uma camada de acesso aos arquivos Parquet sem a necessidade de ingest√£o f√≠sica dos dados no BigQuery, mantendo-os em seu local de origem no GCS.

#### 3.1.1. C√≥digo Principal
```sql
CREATE OR REPLACE EXTERNAL TABLE `ena_raw.tabela_externa_raw_data`
(
    nom_reservatorio STRING,
    cod_resplanejamento STRING,
    tip_reservatorio STRING,
    nom_bacia STRING,
    nom_ree STRING,
    id_subsistema STRING,
    nom_subsistema STRING,
    ena_data STRING,
    ena_bruta_res_mwmed STRING,
    ena_bruta_res_percentualmlt STRING,
    ena_armazenavel_res_mwmed STRING,
    ena_armazenavel_res_percentualmlt STRING,
    ena_queda_bruta STRING,
    mlt_ena STRING
)
OPTIONS(
    format = 'PARQUET',
    uris = ['gs://affable-elf-472819-k2-raw-data/*'],
    description = 'Tabela externa que aponta para os arquivos Parquet no bucket raw-data, com particionamento por data de ingest√£o.'
);
```

#### 3.1.2. Justificativa T√©cnica

A escolha de uma tabela externa para a camada RAW √© estrat√©gica e oferece m√∫ltiplos benef√≠cios para o pipeline de dados:

*   **Separa√ß√£o de Armazenamento e Computa√ß√£o**: A tabela externa permite que os dados permane√ßam no GCS, que √© um servi√ßo de armazenamento de objetos altamente escal√°vel e de baixo custo, enquanto o BigQuery √© utilizado para a camada de computa√ß√£o e consulta. Isso evita a duplica√ß√£o de dados e otimiza os custos de armazenamento.
*   **Flexibilidade e Atualiza√ß√£o Din√¢mica**: Ao referenciar diretamente os arquivos no GCS, qualquer atualiza√ß√£o ou adi√ß√£o de novos arquivos Parquet no bucket √© automaticamente refletida na tabela externa, sem a necessidade de recarregar ou reprocessar os dados no BigQuery. Isso simplifica a manuten√ß√£o do pipeline e garante que a vis√£o dos dados seja sempre a mais recente.
*   **Formato Parquet**: A especifica√ß√£o do `format = 'PARQUET'` √© uma escolha acertada para dados de big data. O Parquet √© um formato de arquivo colunar que oferece alta compress√£o e codifica√ß√£o eficiente, resultando em menor volume de dados para leitura e, consequentemente, em consultas mais r√°pidas e econ√¥micas no BigQuery. Al√©m disso, suporta schemas evolutivos, o que √© ben√©fico em ambientes de dados din√¢micos.
*   **URIs com Wildcard**: O uso de `uris = ['gs://affable-elf-472819-k2-raw-data/*']` √© fundamental para a flexibilidade do pipeline. O wildcard `*` permite que a tabela externa abranja todos os subdiret√≥rios e arquivos dentro do bucket `affable-elf-472819-k2-raw-data`. Isso √© particularmente √∫til quando os dados s√£o particionados por data (e.g., `dt=YYYY-MM-DD`), pois o BigQuery pode inferir essas parti√ß√µes e otimizar as consultas, lendo apenas os dados relevantes para uma determinada data ou per√≠odo.
*   **Campos como STRING**: Inicialmente, todos os campos s√£o definidos como `STRING`. Essa abordagem √© uma boa pr√°tica para a camada RAW, pois preserva os dados em seu formato original, mesmo que inconsistentes (e.g., n√∫meros com v√≠rgula como separador decimal). As transforma√ß√µes de tipo de dado e limpeza ser√£o realizadas em etapas posteriores do pipeline, garantindo que a camada RAW seja uma representa√ß√£o fiel da fonte de dados.

#### 3.1.3. Sugest√µes de Melhoria

Embora a implementa√ß√£o atual seja funcional e siga boas pr√°ticas, algumas melhorias podem ser consideradas para otimizar ainda mais a tabela externa e o pipeline:

*   **Particionamento por Hive**: A inclus√£o da op√ß√£o `hive_partitioning_mode = 'AUTO'` nas `OPTIONS` da tabela externa permitiria que o BigQuery inferisse automaticamente as parti√ß√µes baseadas na estrutura de diret√≥rios do GCS (e.g., `dt=YYYY-MM-DD`). Isso otimizaria as consultas que filtram por data, pois o BigQuery seria capaz de pular diret√≥rios n√£o relevantes, reduzindo a quantidade de dados escaneados e, consequentemente, os custos e o tempo de execu√ß√£o. Atualmente, o filtro √© feito via `_FILE_NAME` na `MERGE` statement, o que √© funcional, mas a infer√™ncia de parti√ß√£o pelo BigQuery √© mais eficiente.
*   **Filtro de Parti√ß√£o Obrigat√≥rio**: Adicionar `require_partition_filter = TRUE` nas `OPTIONS` for√ßaria os usu√°rios a incluir uma cl√°usula `WHERE` que filtre por parti√ß√£o em suas consultas √† tabela externa. Isso √© uma medida de governan√ßa de dados que ajuda a evitar varreduras completas da tabela (full table scans) acidentais, o que pode ser custoso em grandes volumes de dados. Embora o procedimento armazenado j√° filtre por `_FILE_NAME`, essa op√ß√£o adicionaria uma camada de seguran√ßa para consultas ad-hoc.
*   **Valida√ß√£o de Schema**: Para garantir a consist√™ncia dos dados, especialmente em um ambiente onde a fonte de dados pode variar, a implementa√ß√£o de valida√ß√µes de schema no GCS (por exemplo, usando ferramentas como o Apache Avro ou Parquet com schema definido) antes da ingest√£o poderia ser ben√©fica. Embora o BigQuery seja tolerante a schemas, a valida√ß√£o antecipada pode prevenir erros a jusante no pipeline.

Essas melhorias visam aumentar a efici√™ncia, a governan√ßa e a robustez da camada RAW, garantindo que o pipeline seja ainda mais otimizado para cen√°rios de big data.



### 3.2. Procedimento Armazenado (`CREATE_PROCEDURE.sql`)

Este script SQL define um procedimento armazenado no BigQuery, denominado `ena_trusted.sp_atualizar_ena_trusted`, que √© o cora√ß√£o do processo de transforma√ß√£o e carga de dados da camada RAW para a camada TRUSTED. Ele implementa uma l√≥gica de *upsert* (atualiza√ß√£o ou inser√ß√£o) utilizando a instru√ß√£o `MERGE`, garantindo a idempot√™ncia e a integridade dos dados na tabela final.

#### 3.2.1. C√≥digo Principal
```sql
CREATE OR REPLACE PROCEDURE `affable-elf-472819-k2.ena_trusted.sp_atualizar_ena_trusted`(p_data_processamento DATE)
BEGIN
  MERGE `affable-elf-472819-k2.ena_trusted.tabela_ena_trusted` AS T
  USING (
    SELECT
      nom_reservatorio,
      SAFE_CAST(cod_resplanejamento AS INT64) AS cod_resplanejamento,
      tip_reservatorio,
      nom_bacia,
      nom_ree,
      SAFE_CAST(id_subsistema AS INT64) AS id_subsistema,
      nom_subsistema,
      SAFE_CAST(ena_data AS DATE) AS ena_data,
      SAFE_CAST(REPLACE(ena_bruta_res_mwmed, ",", ".") AS FLOAT64) AS ena_bruta_res_mwmed,
      SAFE_CAST(REPLACE(ena_bruta_res_percentualmlt, ",", ".") AS FLOAT64) AS ena_bruta_res_percentualmlt,
      SAFE_CAST(REPLACE(ena_armazenavel_res_mwmed, ",", ".") AS FLOAT64) AS ena_armazenavel_res_mwmed,
      SAFE_CAST(REPLACE(ena_armazenavel_res_percentualmlt, ",", ".") AS FLOAT64) AS ena_armazenavel_res_percentualmlt,
      SAFE_CAST(REPLACE(ena_queda_bruta, ",", ".") AS FLOAT64) AS ena_queda_bruta,
      SAFE_CAST(REPLACE(mlt_ena, ",", ".") AS FLOAT64) AS mlt_ena,
      p_data_processamento AS dt_processamento,
      CURRENT_TIMESTAMP() AS dt_carga
    FROM
      `ena_raw.tabela_externa_raw_data`
    WHERE STARTS_WITH(_FILE_NAME, FORMAT("gs://affable-elf-472819-k2-raw-data/dt=%t", p_data_processamento))
  ) AS S
  ON T.cod_resplanejamento = S.cod_resplanejamento AND T.ena_data = S.ena_data

  WHEN MATCHED THEN
    UPDATE SET
      T.nom_reservatorio = S.nom_reservatorio,
      T.tip_reservatorio = S.tip_reservatorio,
      T.nom_bacia = S.nom_bacia,
      T.nom_ree = S.nom_ree,
      T.id_subsistema = S.id_subsistema,
      T.nom_subsistema = S.nom_subsistema,
      T.ena_bruta_res_mwmed = S.ena_bruta_res_mwmed,
      T.ena_bruta_res_percentualmlt = S.ena_bruta_res_percentualmlt,
      T.ena_armazenavel_res_mwmed = S.ena_armazenavel_res_mwmed,
      T.ena_armazenavel_res_percentualmlt = S.ena_armazenavel_res_percentualmlt,
      T.ena_queda_bruta = S.ena_queda_bruta,
      T.mlt_ena = S.mlt_ena,
      T.dt_carga = S.dt_carga

  WHEN NOT MATCHED THEN
    INSERT (
      nom_reservatorio, cod_resplanejamento, tip_reservatorio, nom_bacia, nom_ree, id_subsistema, nom_subsistema,
      ena_data, ena_bruta_res_mwmed, ena_bruta_res_percentualmlt, ena_armazenavel_res_mwmed,
      ena_armazenavel_res_percentualmlt, ena_queda_bruta, mlt_ena, dt_processamento, dt_carga
    )
    VALUES (
      S.nom_reservatorio, S.cod_resplanejamento, S.tip_reservatorio, S.nom_bacia, S.nom_ree, S.id_subsistema, S.nom_subsistema,
      S.ena_data, S.ena_bruta_res_mwmed, S.ena_bruta_res_percentualmlt, S.ena_armazenavel_res_mwmed,
      S.ena_armazenavel_res_percentualmlt, S.ena_queda_bruta, S.mlt_ena, S.dt_processamento, S.dt_carga
    );
END;
```

#### 3.2.2. Justificativa T√©cnica

O procedimento armazenado `sp_atualizar_ena_trusted` √© crucial para a qualidade e efici√™ncia do pipeline, e suas escolhas de design s√£o bem fundamentadas:

*   **Instru√ß√£o `MERGE` para Upsert**: A utiliza√ß√£o da instru√ß√£o `MERGE` √© uma pr√°tica recomendada para opera√ß√µes de *upsert*, que combinam `INSERT` e `UPDATE` em uma √∫nica transa√ß√£o. Isso garante que, para cada registro de entrada, se j√° existir um registro correspondente na tabela de destino (identificado pela chave de correspond√™ncia `T.cod_resplanejamento = S.cod_resplanejamento AND T.ena_data = S.ena_data`), ele ser√° atualizado. Caso contr√°rio, um novo registro ser√° inserido. Essa abordagem √© fundamental para a idempot√™ncia do pipeline, ou seja, a execu√ß√£o repetida do procedimento com os mesmos dados de entrada n√£o causar√° duplica√ß√µes ou inconsist√™ncias, apenas garantir√° que a tabela `trusted` reflita o estado mais recente dos dados para uma dada data de ENA e reservat√≥rio.
*   **Transforma√ß√µes de Tipo de Dados e Limpeza**: A subquery `USING` realiza as transforma√ß√µes necess√°rias para converter os dados da camada RAW (onde todos os campos s√£o `STRING`) para os tipos de dados apropriados na camada TRUSTED. As fun√ß√µes `SAFE_CAST` s√£o empregadas para lidar com poss√≠veis erros de convers√£o de forma robusta, retornando `NULL` em vez de falhar a consulta inteira se um valor n√£o puder ser convertido. A fun√ß√£o `REPLACE(",", ".")` √© utilizada para padronizar o separador decimal de v√≠rgula para ponto, um requisito comum ao lidar com dados num√©ricos em diferentes formatos regionais. Isso garante que os dados num√©ricos sejam armazenados corretamente como `FLOAT64` e `INT64`, permitindo c√°lculos precisos e an√°lises eficientes.
*   **Filtro Otimizado por `_FILE_NAME`**: A cl√°usula `WHERE STARTS_WITH(_FILE_NAME, FORMAT("gs://affable-elf-472819-k2-raw-data/dt=%t", p_data_processamento))` √© uma t√©cnica de otimiza√ß√£o inteligente. Em vez de escanear toda a tabela externa, que pode conter dados de v√°rias datas, o procedimento filtra os arquivos no GCS com base no nome do arquivo (que inclui a parti√ß√£o `dt=YYYY-MM-DD`). Isso reduz significativamente a quantidade de dados lidos do GCS e processados pelo BigQuery, resultando em consultas mais r√°pidas e custos mais baixos, especialmente em pipelines incrementais que processam dados de um dia espec√≠fico.
*   **Par√¢metro `p_data_processamento`**: A aceita√ß√£o de `p_data_processamento DATE` como par√¢metro permite que o procedimento seja flex√≠vel e reutiliz√°vel. Ele pode ser invocado para processar dados de qualquer data espec√≠fica, facilitando o reprocessamento de dados hist√≥ricos ou o processamento di√°rio de forma controlada. Al√©m disso, a inclus√£o de `dt_processamento` e `dt_carga` na tabela `trusted` fornece metadados importantes para auditoria e rastreabilidade, indicando quando os dados foram processados e carregados.

#### 3.2.3. Sugest√µes de Melhoria

Para aprimorar ainda mais a robustez e a capacidade de monitoramento do procedimento armazenado, as seguintes melhorias podem ser consideradas:

*   **Logging Detalhado**: Implementar um mecanismo de logging mais detalhado dentro do procedimento. Isso pode ser feito inserindo registros em uma tabela de log dedicada (`INSERT INTO log_table ...`) que capture informa√ß√µes como o status da execu√ß√£o (sucesso/falha), a data de processamento, o n√∫mero de registros inseridos/atualizados, e mensagens de erro. Isso √© crucial para depura√ß√£o e monitoramento da sa√∫de do pipeline em produ√ß√£o.
*   **Valida√ß√µes de Dados e Erros**: Adicionar valida√ß√µes expl√≠citas para cen√°rios espec√≠ficos. Por exemplo, verificar se a parti√ß√£o de dados para `p_data_processamento` existe na camada RAW antes de tentar process√°-la. Se a parti√ß√£o estiver vazia ou n√£o existir, o procedimento poderia registrar um aviso ou erro e sair, em vez de tentar processar dados inexistentes. A instru√ß√£o `IF COUNT(*) = 0 THEN RAISE ERROR` pode ser usada para interromper a execu√ß√£o em caso de condi√ß√µes inesperadas, como a aus√™ncia de dados para a data especificada.
*   **Tratamento de Erros Transacionais**: Embora o BigQuery seja transacional por padr√£o para opera√ß√µes DML, considerar blocos `BEGIN...EXCEPTION...END` (se o BigQuery suportar sintaxe similar para procedimentos) para um tratamento de erros mais granular, permitindo *rollback* ou a√ß√µes corretivas espec√≠ficas em caso de falha em partes do `MERGE`.
*   **Modularidade e Reutiliza√ß√£o**: Para procedimentos mais complexos, considerar a cria√ß√£o de fun√ß√µes auxiliares ou sub-procedimentos para encapsular l√≥gicas espec√≠ficas (e.g., valida√ß√£o de um campo espec√≠fico, c√°lculo de uma m√©trica). Isso melhora a legibilidade e a manutenibilidade do c√≥digo SQL.
*   **Vari√°veis e Loops para Batches**: Se houver a necessidade de processar dados em batches menores ou iterar sobre uma lista de datas, o uso de `DECLARE` para vari√°veis e estruturas de loop (como `WHILE` ou `FOR`) pode ser explorado. No entanto, para o cen√°rio atual de processamento di√°rio, a abordagem parametrizada √© eficiente.

Essas sugest√µes visam tornar o procedimento mais observ√°vel, resiliente a dados inesperados e mais f√°cil de depurar e manter em um ambiente de produ√ß√£o.



### 3.3. An√°lise Preliminar de Dados (`AN√ÅLISEPRELIMINAR.sql`)

Este script SQL cont√©m um conjunto de queries projetadas para realizar uma an√°lise preliminar da qualidade dos dados brutos de ENA na camada RAW do BigQuery. O objetivo √© identificar rapidamente potenciais problemas como valores nulos, formatos inv√°lidos, inconsist√™ncias e padr√µes inesperados antes que os dados sejam transformados e carregados na camada TRUSTED. Esta etapa √© fundamental para garantir a confiabilidade dos dados que ser√£o utilizados em an√°lises e modelos de IA.

#### 3.3.1. Exemplos de Queries e Justificativa T√©cnica

O script `AN√ÅLISEPRELIMINAR.sql` aborda diversas dimens√µes da qualidade dos dados:

*   **Contagem Total de Registros e Unicidade**: Queries como `SELECT COUNT(*) as total_registros, COUNT(DISTINCT cod_resplanejamento) as reservatorios_unicos FROM ena_raw.tabela_externa_raw_data;` fornecem uma vis√£o geral do volume de dados e da unicidade de identificadores chave. Isso ajuda a verificar se o volume de dados est√° conforme o esperado e se n√£o h√° duplica√ß√µes inesperadas na granularidade do identificador.

*   **Percentual de Nulos por Coluna**: A an√°lise de nulos, exemplificada por `ROUND(100 * SUM(CASE WHEN nom_reservatorio IS NULL OR nom_reservatorio = '' THEN 1 ELSE 0 END) / COUNT(*), 2) as pct_nom_reservatorio_nulo`, √© crucial para identificar campos com alta taxa de dados ausentes. Valores nulos podem indicar problemas na fonte de dados ou no processo de ingest√£o, e seu tratamento adequado √© essencial para evitar vieses em an√°lises posteriores.

*   **Estat√≠sticas e Formatos Num√©ricos**: Queries que verificam `ena_bruta_res_mwmed` como string e usam `REGEXP_CONTAINS` para validar padr√µes num√©ricos (`r'^-?[0-9]+([,.][0-9]+)?$'`) s√£o importantes para entender a distribui√ß√£o dos valores e identificar registros que n√£o se encaixam no formato num√©rico esperado. Isso √© vital antes de realizar `SAFE_CAST` para tipos num√©ricos, pois valores mal formatados podem levar a `NULL`s ou erros de convers√£o.

*   **Validade e Range Temporal das Datas**: A verifica√ß√£o de datas com `SAFE_CAST(ena_data AS DATE) IS NULL` e a identifica√ß√£o de `MIN`/`MAX` datas ajudam a garantir que os campos de data estejam em um formato v√°lido e que o per√≠odo coberto pelos dados esteja correto. Inconsist√™ncias em datas podem afetar a an√°lise temporal e o particionamento.

*   **Valores √önicos por Categoria**: Queries `UNION ALL` para campos categ√≥ricos como `tip_reservatorio` e `nom_subsistema` permitem entender a variedade e a distribui√ß√£o dos valores. Isso pode revelar erros de digita√ß√£o, categorias inesperadas ou a necessidade de padroniza√ß√£o.

*   **Qualidade por Parti√ß√£o**: A an√°lise da qualidade dos dados por parti√ß√£o (`REGEXP_EXTRACT(_FILE_NAME, r'dt=([^/]+)')`) √© uma pr√°tica excelente para identificar problemas que podem estar isolados em datas espec√≠ficas de ingest√£o. Isso ajuda a rastrear a origem de problemas de qualidade de dados ao longo do tempo.

*   **Amostra Aleat√≥ria para Visualiza√ß√£o**: `TABLESAMPLE SYSTEM (10 PERCENT) LIMIT 50` √© uma ferramenta pr√°tica para obter uma amostra representativa dos dados brutos. Essa amostra pode ser usada para inspe√ß√£o manual, visualiza√ß√£o r√°pida ou para alimentar ferramentas de explora√ß√£o de dados, sem a necessidade de processar o conjunto de dados completo.

*   **Dashboard de Qualidade de Dados**: A query final que agrega m√©tricas como `total_registros`, `reservatorios_unicos`, `pct_ena_bruta_ausente`, `pct_datas_validas` e `pct_codigos_invalidos` serve como um resumo executivo da qualidade dos dados. Isso pode ser a base para um dashboard de monitoramento, fornecendo uma vis√£o r√°pida da sa√∫de do pipeline.

Em suma, a justificativa para estas queries reside na necessidade de **garantir a qualidade dos dados desde a origem**. Ao identificar e quantificar problemas na camada RAW, √© poss√≠vel tomar decis√µes informadas sobre limpeza, transforma√ß√£o e at√© mesmo sobre a necessidade de corre√ß√£o na fonte de dados, antes que esses problemas se propaguem para as camadas mais refinadas do data warehouse.

#### 3.3.2. Sugest√µes de Melhoria

Para elevar o n√≠vel da an√°lise preliminar de dados e integr√°-la de forma mais eficaz ao pipeline, as seguintes melhorias s√£o recomendadas:

*   **Automa√ß√£o via Scheduled Queries no BigQuery**: As queries de an√°lise preliminar podem ser configuradas como *Scheduled Queries* no BigQuery. Isso permitiria a execu√ß√£o autom√°tica di√°ria (ou em outra frequ√™ncia definida) dessas verifica√ß√µes, gerando relat√≥rios de qualidade de dados de forma proativa. Os resultados poderiam ser armazenados em uma tabela de auditoria ou enviados para um sistema de alerta.
*   **Integra√ß√£o com Ferramentas de Data Governance (Dataform/dbt)**: Para um pipeline de dados mais maduro, a integra√ß√£o com ferramentas como Dataform ou dbt (data build tool) seria altamente ben√©fica. Essas ferramentas permitem versionar o c√≥digo SQL, criar testes de qualidade de dados como parte do pipeline de transforma√ß√£o (e.g., `assert not_null(column_name)`, `assert unique(id_column)`), e documentar o lineage dos dados. Isso transformaria a an√°lise preliminar em um componente ativo e automatizado do processo de ETL.
*   **Visualiza√ß√µes Interativas (Looker/Data Studio)**: Os resultados das queries de qualidade de dados podem ser facilmente conectados a ferramentas de visualiza√ß√£o como Looker ou Data Studio. A cria√ß√£o de dashboards interativos permitiria que analistas e stakeholders monitorassem a qualidade dos dados em tempo real, identificassem tend√™ncias de degrada√ß√£o da qualidade e tomassem a√ß√µes corretivas de forma mais √°gil. Por exemplo, um gr√°fico mostrando o percentual de nulos por coluna ao longo do tempo seria extremamente √∫til.
*   **Defini√ß√£o de Limiares e Alertas**: Para cada m√©trica de qualidade de dados (e.g., percentual de nulos, contagem de registros inv√°lidos), poderiam ser definidos limiares. Se uma m√©trica exceder um limiar pr√©-definido, um alerta autom√°tico (via Cloud Monitoring, Pub/Sub, ou e-mail) poderia ser disparado para a equipe de engenharia de dados, indicando um problema que requer aten√ß√£o imediata.
*   **Cria√ß√£o de Views Materializadas para M√©tricas**: Para queries de qualidade de dados que s√£o executadas frequentemente ou que envolvem grandes volumes de dados, a cria√ß√£o de *views materializadas* no BigQuery pode otimizar o desempenho. Essas views pr√©-computam os resultados das queries e os armazenam, acelerando o acesso √†s m√©tricas de qualidade.

Ao implementar essas melhorias, a an√°lise preliminar de dados se tornaria n√£o apenas uma ferramenta de diagn√≥stico, mas um sistema proativo de garantia de qualidade, essencial para a constru√ß√£o de um data lakehouse confi√°vel e para o suporte a aplica√ß√µes de IA que dependem de dados de alta qualidade.



### 3.4. Tabela Trusted (`CREATEENA_TRUSTED.sql`)

Este script SQL √© respons√°vel pela cria√ß√£o ou substitui√ß√£o da tabela `ena_trusted.tabela_ena_trusted` no BigQuery. Esta tabela representa a camada *trusted* (confi√°vel) do pipeline de dados, onde os dados brutos de ENA, ap√≥s passarem por processos de limpeza, valida√ß√£o e transforma√ß√£o, s√£o armazenados em um formato otimizado para an√°lise e consumo por aplica√ß√µes downstream, como relat√≥rios, dashboards e modelos de Machine Learning.

#### 3.4.1. C√≥digo Principal
```sql
CREATE OR REPLACE TABLE `affable-elf-472819-k2.ena_trusted.tabela_ena_trusted`
(
  -- Colunas de Identifica√ß√£o
  nom_reservatorio STRING,
  cod_resplanejamento INT64, -- Convertido de STRING para INT64
  tip_reservatorio STRING,
  nom_bacia STRING,
  nom_ree STRING,
  id_subsistema INT64, -- Convertido de STRING para INT64
  nom_subsistema STRING,

  -- Colunas de Dados e Valores
  ena_bruta_res_mwmed FLOAT64, -- Convertido de STRING para FLOAT64
  ena_bruta_res_percentualmlt FLOAT64, -- Convertido de STRING para FLOAT64
  ena_armazenavel_res_mwmed FLOAT64, -- Convertido de STRING para FLOAT64
  ena_armazenavel_res_percentualmlt FLOAT64, -- Convertido de STRING para FLOAT64
  ena_queda_bruta FLOAT64, -- Convertido de STRING para FLOAT64
  mlt_ena FLOAT64, -- Convertido de STRING para FLOAT64

  -- Colunas de Controle e Particionamento
  ena_data DATE, -- Convertido de STRING para DATE
  dt_processamento DATE,
  dt_carga TIMESTAMP
)
PARTITION BY
  dt_processamento
OPTIONS(
  description="Tabela Trusted com dados de ENA por reservat√≥rio, particionada por data de processamento."
);
```

#### 3.4.2. Justificativa T√©cnica

A estrutura e as op√ß√µes definidas para a `tabela_ena_trusted` refletem as melhores pr√°ticas para um data warehouse anal√≠tico no BigQuery:

*   **Tipos de Dados Convertidos e Otimizados**: Ao contr√°rio da camada RAW, onde os dados s√£o mantidos como `STRING` para flexibilidade, na camada TRUSTED, os campos s√£o convertidos para seus tipos de dados nativos e mais apropriados (e.g., `INT64`, `FLOAT64`, `DATE`, `TIMESTAMP`). Essa convers√£o √© fundamental por v√°rias raz√µes:
    *   **Otimiza√ß√£o de Armazenamento e Desempenho**: Tipos de dados corretos permitem que o BigQuery armazene os dados de forma mais eficiente e execute opera√ß√µes (filtros, agrega√ß√µes, joins) de maneira mais r√°pida, pois n√£o h√° necessidade de convers√µes impl√≠citas em tempo de consulta.
    *   **Precis√£o Anal√≠tica**: Garante que c√°lculos num√©ricos e opera√ß√µes de data/hora sejam realizados com precis√£o, evitando erros que poderiam surgir de manipula√ß√µes de strings.
    *   **Consist√™ncia e Valida√ß√£o**: Refor√ßa a consist√™ncia dos dados, pois apenas valores que podem ser convertidos para o tipo especificado s√£o aceitos, atuando como uma forma de valida√ß√£o de schema.

*   **Particionamento por `dt_processamento`**: A cl√°usula `PARTITION BY dt_processamento` √© uma t√©cnica de otimiza√ß√£o essencial no BigQuery. Ela organiza a tabela em parti√ß√µes menores com base na data de processamento. Os benef√≠cios incluem:
    *   **Redu√ß√£o de Custos**: Quando as consultas filtram por `dt_processamento`, o BigQuery escaneia apenas as parti√ß√µes relevantes, reduzindo a quantidade de dados processados e, consequentemente, os custos de consulta.
    *   **Melhora de Desempenho**: Consultas que acessam um subconjunto de dados particionados s√£o executadas mais rapidamente, pois o BigQuery pode eliminar parti√ß√µes n√£o necess√°rias.
    *   **Gerenciamento de Dados**: Facilita a gest√£o do ciclo de vida dos dados, permitindo a exclus√£o ou expira√ß√£o de parti√ß√µes antigas de forma eficiente.

*   **Clustering Impl√≠cito e Otimiza√ß√£o do BigQuery**: Embora o script n√£o especifique explicitamente `CLUSTER BY`, o BigQuery aplica otimiza√ß√µes de armazenamento e consulta automaticamente. Para tabelas particionadas, o BigQuery pode organizar os dados dentro de cada parti√ß√£o de forma a otimizar o acesso para colunas frequentemente usadas em filtros ou jun√ß√µes (como `cod_resplanejamento` e `ena_data`). Isso √© particularmente eficaz para padr√µes de acesso que envolvem a recupera√ß√£o de dados para reservat√≥rios espec√≠ficos em datas espec√≠ficas.

*   **Descri√ß√£o da Tabela**: A `OPTIONS(description="...")` √© uma boa pr√°tica de documenta√ß√£o de dados. Ela fornece metadados importantes sobre a finalidade da tabela, facilitando a compreens√£o e o uso por outros engenheiros de dados, analistas e cientistas de dados.

#### 3.4.3. Sugest√µes de Melhoria

Para aprimorar ainda mais a performance e a usabilidade da tabela `ena_trusted`, as seguintes melhorias podem ser consideradas:

*   **Clustering Expl√≠cito**: Embora o BigQuery otimize automaticamente, a adi√ß√£o de `CLUSTER BY cod_resplanejamento, ena_data` pode refinar ainda mais a organiza√ß√£o f√≠sica dos dados dentro de cada parti√ß√£o. O clustering organiza os dados com base nos valores das colunas especificadas, o que pode acelerar consultas que filtram ou agregam por essas colunas, especialmente quando elas s√£o frequentemente usadas em conjunto com o particionamento. A escolha das colunas para clustering deve ser baseada nos padr√µes de consulta mais comuns.
*   **Defini√ß√£o de Chave Prim√°ria e √önica (Conceitual)**: O BigQuery n√£o imp√µe chaves prim√°rias ou restri√ß√µes de unicidade no n√≠vel do schema como bancos de dados relacionais tradicionais. No entanto, √© uma boa pr√°tica documentar conceitualmente quais colunas formam a chave prim√°ria (e.g., `cod_resplanejamento` e `ena_data`) e garantir sua unicidade atrav√©s da l√≥gica do procedimento `MERGE`. Isso √© vital para a integridade referencial e para evitar duplica√ß√µes l√≥gicas.
*   **√çndices ou Views Materializadas para Consultas Comuns**: Para consultas de alto desempenho que s√£o executadas com muita frequ√™ncia e envolvem agrega√ß√µes complexas ou jun√ß√µes, a cria√ß√£o de *views materializadas* pode ser uma solu√ß√£o eficaz. Elas pr√©-computam os resultados de uma consulta e os armazenam, acelerando significativamente o tempo de resposta. Alternativamente, para cen√°rios espec√≠ficos, a cria√ß√£o de tabelas agregadas ou sumarizadas pode ser mais apropriada.
*   **Pol√≠ticas de Expira√ß√£o de Dados**: Para gerenciar o custo de armazenamento e a conformidade com pol√≠ticas de reten√ß√£o de dados, configurar pol√≠ticas de expira√ß√£o de dados para a tabela ou para parti√ß√µes espec√≠ficas. Isso pode ser feito via `DEFAULT TABLE EXPIRATION` ou `PARTITION EXPIRATION` nas op√ß√µes da tabela, garantindo que dados antigos sejam automaticamente removidos.
*   **Controle de Acesso Granular**: Refor√ßar o controle de acesso (IAM) na tabela `trusted`, concedendo permiss√µes de leitura apenas para usu√°rios e servi√ßos que realmente precisam acessar esses dados, e permiss√µes de escrita apenas para o servi√ßo que executa o procedimento de atualiza√ß√£o (neste caso, a Cloud Function atrav√©s de sua conta de servi√ßo).

Essas melhorias visam transformar a `tabela_ena_trusted` em um ativo de dados ainda mais valioso, otimizado para desempenho, governan√ßa e seguran√ßa, e pronto para suportar as demandas anal√≠ticas mais exigentes.



### 3.5. Cloud Function (C√≥digo Python)

A Cloud Function √© o componente de orquestra√ß√£o serverless que atua como o gatilho para a execu√ß√£o do procedimento armazenado no BigQuery. Desenvolvida em Python, esta fun√ß√£o √© acionada via HTTP e √© respons√°vel por receber requisi√ß√µes, extrair par√¢metros de data de processamento e invocar o procedimento `sp_atualizar_ena_trusted` no BigQuery. Sua natureza serverless a torna ideal para tarefas de orquestra√ß√£o e automa√ß√£o, sem a necessidade de gerenciar servidores.

#### 3.5.1. C√≥digo Principal
```python
import functions_framework
from google.cloud import bigquery
from flask import jsonify
import logging

@functions_framework.http
def run_bigquery_procedure(request):
    """
    Executa o procedimento armazenado do BigQuery para atualizar a tabela ENA.

    A data de processamento √© passada como par√¢metro para o procedimento.
    1. No body (JSON): { "data": "2025-09-24" }
    2. Na query string: ?data=2025-09-24

    Caso n√£o seja enviada, usa CURRENT_DATE().
    """
    # Configura o logging
    logging.basicConfig(level=logging.INFO)

    try:
        # 1. Verifica se veio na query string
        data_param = request.args.get("data")

        # 2. Se n√£o veio na query, tenta pegar do body JSON
        if not data_param:
            request_json = request.get_json(silent=True) or {}
            data_param = request_json.get("data")

        # Configura√ß√µes do BigQuery
        client = bigquery.Client()
        project_dataset_procedure = '`affable-elf-472819-k2.ena_trusted.sp_atualizar_ena_trusted`'
        data_usada = ''
        
        # 3. Monta o SQL
        if data_param:
            # CORRE√á√ÉO: Passa a data como uma string literal entre aspas simples ('YYYY-MM-DD').
            # O BigQuery converte essa string para DATE, que √© o tipo esperado pelo procedimento.
            sql_command = f"""
                CALL {project_dataset_procedure}('{data_param}');
            """
            data_usada = data_param
        else:
            # Usa CURRENT_DATE() quando nenhuma data √© fornecida
            sql_command = f"""
                CALL {project_dataset_procedure}(CURRENT_DATE());
            """
            data_usada = "CURRENT_DATE()"

        logging.info(f"Executando SQL: {sql_command.strip()}")
        
        # Execu√ß√£o do procedimento
        query_job = client.query(sql_command)
        query_job.result()  # Aguarda execu√ß√£o terminar
        
        logging.info("Procedimento executado com sucesso.")
        return jsonify({
            "status": "sucesso",
            "mensagem": "Procedimento BigQuery executado com sucesso.",
            "data_processamento": data_usada
        }), 200
        
    except Exception as e:
        # Loga o erro completo para diagn√≥stico
        logging.error(f"Erro ao executar o procedimento: {e}", exc_info=True)
        return jsonify({
            "status": "erro",
            "mensagem": f"Erro interno ao executar o procedimento BigQuery. Detalhe: {str(e)}"
        }), 500
```

#### 3.5.2. Justificativa T√©cnica

A implementa√ß√£o da Cloud Function demonstra uma abordagem eficiente e flex√≠vel para a orquestra√ß√£o do pipeline:

*   **Serverless e Escalabilidade Autom√°tica**: A principal vantagem de usar Cloud Functions √© o modelo serverless. Isso significa que n√£o h√° servidores para provisionar, gerenciar ou escalar. O GCP cuida de toda a infraestrutura subjacente, permitindo que a fun√ß√£o escale automaticamente para lidar com o volume de requisi√ß√µes, desde execu√ß√µes espor√°dicas at√© picos de demanda. O desenvolvedor pode focar apenas na l√≥gica de neg√≥cio.
*   **Gatilho HTTP Flex√≠vel**: A fun√ß√£o √© exposta via um gatilho HTTP (`@functions_framework.http`), o que a torna extremamente vers√°til. Ela pode ser invocada manualmente (para testes ou reprocessamento), por outras aplica√ß√µes, por servi√ßos de agendamento (como Cloud Scheduler para execu√ß√µes di√°rias), ou at√© mesmo por webhooks. Essa flexibilidade de invoca√ß√£o √© um ponto forte do design.
*   **Flexibilidade na Entrada de Dados**: A fun√ß√£o inteligentemente aceita o par√¢metro `data` tanto via *query string* (`request.args.get(


data")`) quanto via corpo da requisi√ß√£o JSON (`request.get_json().get("data")`). Al√©m disso, ela fornece um *fallback* para `CURRENT_DATE()` do BigQuery caso nenhuma data seja fornecida. Essa flexibilidade torna a fun√ß√£o mais amig√°vel e robusta para diferentes cen√°rios de uso.
*   **Integra√ß√£o Direta com BigQuery**: A fun√ß√£o utiliza a biblioteca cliente oficial do Google Cloud para Python (`google.cloud.bigquery`) para interagir com o BigQuery. Isso garante uma comunica√ß√£o segura e eficiente com o servi√ßo. A chamada `client.query(sql_command)` envia a instru√ß√£o SQL para o BigQuery, e `query_job.result()` garante que a fun√ß√£o aguarde a conclus√£o da execu√ß√£o do procedimento armazenado antes de retornar uma resposta, tornando a opera√ß√£o s√≠ncrona do ponto de vista da Cloud Function.
*   **Tratamento de Erros e Logging**: A inclus√£o de um bloco `try-except` e o uso do m√≥dulo `logging` s√£o cruciais para a robustez da fun√ß√£o. O `try-except` captura quaisquer exce√ß√µes que possam ocorrer durante a execu√ß√£o do procedimento BigQuery ou na manipula√ß√£o dos par√¢metros, retornando uma resposta de erro HTTP 500 com uma mensagem descritiva. O `logging.basicConfig(level=logging.INFO)` e as chamadas `logging.info()` e `logging.error()` permitem que os eventos e erros da fun√ß√£o sejam registrados no Cloud Logging, facilitando o monitoramento, a depura√ß√£o e a auditoria da execu√ß√£o do pipeline.
*   **Resposta Padronizada**: A fun√ß√£o retorna uma resposta JSON padronizada, indicando o `status` (sucesso ou erro), uma `mensagem` e a `data_processamento` utilizada. Isso facilita a integra√ß√£o com outros sistemas e o consumo da resposta por ferramentas de monitoramento ou orquestra√ß√£o.

#### 3.5.3. Sugest√µes de Melhoria

Para tornar a Cloud Function ainda mais robusta, segura e eficiente em um ambiente de produ√ß√£o, as seguintes melhorias s√£o recomendadas:

*   **Autentica√ß√£o e Autoriza√ß√£o**: Atualmente, a fun√ß√£o √© acess√≠vel publicamente via HTTP. Para ambientes de produ√ß√£o, √© crucial implementar mecanismos de autentica√ß√£o e autoriza√ß√£o. Isso pode ser feito de v√°rias maneiras:
    *   **Cloud IAM**: Restringir o acesso √† fun√ß√£o apenas a contas de servi√ßo ou usu√°rios espec√≠ficos via pol√≠ticas do IAM.
    *   **API Gateway**: Colocar um API Gateway na frente da Cloud Function para gerenciar autentica√ß√£o (e.g., chaves de API, OAuth 2.0) e autoriza√ß√£o, al√©m de fornecer recursos como limita√ß√£o de taxa e caching.
    *   **Token de ID**: Para invoca√ß√µes internas entre servi√ßos GCP, usar tokens de ID para autenticar chamadas de servi√ßo para servi√ßo.
*   **Execu√ß√£o Ass√≠ncrona para Jobs Longos**: Para procedimentos BigQuery que podem levar um tempo consider√°vel para serem conclu√≠dos, a abordagem s√≠ncrona (`query_job.result()`) pode fazer com que a Cloud Function exceda seu tempo limite de execu√ß√£o (timeout). Uma melhoria seria iniciar o job do BigQuery de forma ass√≠ncrona e retornar imediatamente um `job_id` para o cliente. O cliente (ou outro servi√ßo) poderia ent√£o consultar o status do job usando esse `job_id`. Isso libera a Cloud Function rapidamente e permite que jobs de longa dura√ß√£o sejam executados em segundo plano.
*   **Monitoramento e Alertas Aprimorados**: Al√©m do logging b√°sico, integrar a fun√ß√£o com o Cloud Monitoring para criar m√©tricas personalizadas (e.g., tempo de execu√ß√£o do procedimento, n√∫mero de vezes que o fallback para `CURRENT_DATE()` foi usado, contagem de erros). Configurar alertas baseados nessas m√©tricas para notificar a equipe de opera√ß√µes sobre falhas ou comportamentos anormais.
*   **Configura√ß√£o de Vari√°veis de Ambiente**: Evitar o *hardcoding* de valores como `project_dataset_procedure` diretamente no c√≥digo. Em vez disso, utilizar vari√°veis de ambiente (e.g., `os.environ.get('BIGQUERY_PROCEDURE_PATH')`). Isso torna a fun√ß√£o mais flex√≠vel, f√°cil de configurar para diferentes ambientes (desenvolvimento, staging, produ√ß√£o) e melhora a seguran√ßa, pois credenciais ou IDs de projeto n√£o ficam expostos no c√≥digo-fonte.
*   **Testes Unit√°rios e de Integra√ß√£o**: Desenvolver testes unit√°rios para a l√≥gica da Cloud Function (e.g., parsing de par√¢metros, constru√ß√£o da query SQL) utilizando frameworks como `pytest`. Utilizar *mocks* para simular as intera√ß√µes com o cliente BigQuery, garantindo que a l√≥gica da fun√ß√£o funcione conforme o esperado sem a necessidade de executar chamadas reais ao BigQuery. Testes de integra√ß√£o tamb√©m seriam valiosos para verificar a comunica√ß√£o ponta a ponta com o BigQuery.
*   **Cloud Scheduler para Agendamento**: Para automa√ß√£o di√°ria ou em intervalos regulares, configurar um job no Cloud Scheduler que invoque a Cloud Function via HTTP. Isso elimina a necessidade de invoca√ß√µes manuais e garante que o pipeline seja executado de forma consistente no hor√°rio desejado.
*   **Valida√ß√£o de Entrada Mais Robusta**: Embora a fun√ß√£o j√° lide com a aus√™ncia de `data_param`, uma valida√ß√£o mais rigorosa do formato da data (se fornecida) poderia ser implementada para evitar que datas mal formatadas cheguem ao BigQuery e causem erros no procedimento armazenado. Isso pode ser feito com bibliotecas como `datetime` do Python.

Ao implementar essas melhorias, a Cloud Function se tornar√° um componente ainda mais confi√°vel e de n√≠vel de produ√ß√£o, capaz de suportar as demandas de um pipeline de dados cr√≠tico.



## 4. An√°lise T√©cnica Detalhada e Justificativas Adicionais

### 4.1. Arquitetura Geral e Fluxo de Dados: Uma Vis√£o Integrada

A arquitetura do pipeline de dados ENA √© um exemplo cl√°ssico de um design moderno de data lakehouse no Google Cloud Platform, combinando a flexibilidade de um data lake (GCS para dados RAW) com as capacidades de processamento e an√°lise de um data warehouse (BigQuery para dados TRUSTED). A orquestra√ß√£o via Cloud Functions adiciona a agilidade e a escalabilidade necess√°rias para um pipeline de dados eficiente.

O fluxo de dados pode ser visualizado como uma jornada em tr√™s est√°gios principais:

1.  **Ingest√£o e Armazenamento RAW (GCS + BigQuery External Table)**: Os dados de ENA, tipicamente gerados em sistemas externos, s√£o primeiramente carregados no Google Cloud Storage. A escolha do GCS como camada de armazenamento RAW √© estrat√©gica devido √† sua durabilidade, escalabilidade ilimitada e baixo custo. Os arquivos s√£o armazenados em formato Parquet, que √© um formato colunar otimizado para an√°lise, permitindo compress√£o eficiente e leitura seletiva de colunas, o que reduz o volume de dados a serem processados. A organiza√ß√£o dos dados em diret√≥rios particionados por data (`dt=YYYY-MM-DD`) no GCS √© uma pr√°tica fundamental para otimiza√ß√£o de custos e desempenho em consultas futuras. A tabela externa do BigQuery (`ena_raw.tabela_externa_raw_data`) atua como um *schema-on-read* sobre esses arquivos, permitindo que o BigQuery os consulte diretamente sem a necessidade de ingest√£o f√≠sica. Isso oferece flexibilidade, pois o schema pode ser adaptado conforme a necessidade sem alterar os dados subjacentes, e evita a duplica√ß√£o de dados, mantendo uma √∫nica fonte de verdade para os dados brutos.

2.  **Transforma√ß√£o e Enriquecimento (BigQuery Stored Procedure)**: A fase de transforma√ß√£o √© orquestrada pelo procedimento armazenado `sp_atualizar_ena_trusted` no BigQuery. Este procedimento √© o motor do pipeline, respons√°vel por converter os dados brutos (strings) em tipos de dados apropriados (inteiros, floats, datas), padronizar formatos (e.g., substitui√ß√£o de v√≠rgulas por pontos em n√∫meros decimais) e aplicar qualquer l√≥gica de neg√≥cio necess√°ria para preparar os dados para an√°lise. A instru√ß√£o `MERGE` √© central aqui, pois permite a implementa√ß√£o de uma l√≥gica de *upsert* eficiente. Isso significa que, para cada registro de ENA para um dado reservat√≥rio e data, o procedimento verifica se o registro j√° existe na tabela `ena_trusted`. Se existir, ele √© atualizado; caso contr√°rio, √© inserido. Essa abordagem garante a idempot√™ncia do pipeline, prevenindo duplica√ß√µes e mantendo a tabela `trusted` sempre atualizada com a vers√£o mais recente e limpa dos dados. O filtro por `_FILE_NAME` dentro do procedimento √© uma otimiza√ß√£o crucial, garantindo que apenas os dados da parti√ß√£o relevante no GCS sejam lidos, minimizando o volume de dados escaneados e, consequentemente, os custos e o tempo de execu√ß√£o.

3.  **Armazenamento e Consumo TRUSTED (BigQuery Partitioned Table)**: Ap√≥s a transforma√ß√£o, os dados s√£o armazenados na `ena_trusted.tabela_ena_trusted`. Esta tabela √© projetada para ser a fonte de dados para todas as aplica√ß√µes downstream. Ao contr√°rio da camada RAW, os dados aqui j√° est√£o limpos, tipados corretamente e prontos para uso. O particionamento da tabela por `dt_processamento` (data em que o registro foi processado e carregado na camada trusted) √© uma otimiza√ß√£o chave. Isso permite que as consultas que filtram por data (um padr√£o de uso muito comum em an√°lises de s√©ries temporais) escaneiem apenas um subconjunto dos dados, resultando em desempenho superior e custos reduzidos. A estrutura da tabela `trusted` √© otimizada para consultas anal√≠ticas, com tipos de dados que permitem opera√ß√µes num√©ricas e temporais eficientes, tornando-a ideal para relat√≥rios, dashboards e como base para modelos de Machine Learning.

### 4.2. Justificativa das Escolhas Tecnol√≥gicas (GCP)

As escolhas de tecnologias do Google Cloud Platform (GCP) para este projeto n√£o foram arbitr√°rias, mas sim fundamentadas em uma an√°lise cuidadosa dos requisitos de escalabilidade, custo, manuten√ß√£o e integra√ß√£o. A combina√ß√£o de Cloud Storage, BigQuery e Cloud Functions oferece uma solu√ß√£o robusta e moderna para o pipeline de dados de ENA.

*   **Google Cloud Storage (GCS) para Data Lake**: O GCS √© a escolha natural para um data lake devido √† sua **escalabilidade ilimitada**, **durabilidade de 99.999999999% (11 noves)** e **custo-benef√≠cio** para armazenamento de grandes volumes de dados. Ele suporta diversos formatos de arquivo, sendo o Parquet uma excelente op√ß√£o para dados estruturados. A integra√ß√£o nativa com o BigQuery via tabelas externas simplifica a arquitetura, eliminando a necessidade de mover dados para o BigQuery para consulta, o que seria custoso e demorado.

*   **BigQuery para Data Warehouse e Processamento**: O BigQuery √© um data warehouse anal√≠tico **serverless e altamente escal√°vel**, projetado para lidar com petabytes de dados. Suas principais vantagens incluem:
    *   **Desempenho Inigual√°vel**: Capaz de executar consultas complexas sobre enormes volumes de dados em segundos, gra√ßas √† sua arquitetura colunar e processamento massivamente paralelo.
    *   **Gerenciamento Zero de Infraestrutura**: Elimina a necessidade de provisionar, escalar ou manter servidores, permitindo que a equipe se concentre na an√°lise de dados em vez da administra√ß√£o de banco de dados.
    *   **Modelo de Custos Otimizado**: O modelo de pagamento por dados escaneados incentiva a otimiza√ß√£o de consultas e o uso de particionamento/clustering, resultando em custos previs√≠veis e control√°veis.
    *   **Suporte a SQL Padr√£o**: Facilita a ado√ß√£o por equipes familiarizadas com SQL e permite a cria√ß√£o de procedimentos armazenados complexos para l√≥gica de ETL.
    *   **Ecossistema Rico**: Integra-se perfeitamente com outras ferramentas GCP, como Data Studio, Looker e Vertex AI, para visualiza√ß√£o e Machine Learning.

*   **Cloud Functions para Orquestra√ß√£o Serverless**: A Cloud Function √© a cola que une o GCS e o BigQuery neste pipeline. Suas vantagens s√£o:
    *   **Serverless e Pagamento por Uso**: Executa c√≥digo em resposta a eventos HTTP sem a necessidade de gerenciar servidores, cobrando apenas pelo tempo de execu√ß√£o e n√∫mero de invoca√ß√µes. Isso √© ideal para tarefas de orquestra√ß√£o que podem ser disparadas sob demanda ou agendadas.
    *   **Desenvolvimento √Ågil**: Permite o desenvolvimento r√°pido de fun√ß√µes em Python (ou outras linguagens), com foco na l√≥gica de neg√≥cio espec√≠fica para orquestrar o pipeline.
    *   **Integra√ß√£o com o Ecossistema GCP**: Facilita a intera√ß√£o com outros servi√ßos GCP, como BigQuery, Cloud Storage e Cloud Scheduler, atrav√©s das bibliotecas cliente oficiais.
    *   **Manuten√ß√£o Simplificada**: Reduz a carga operacional, pois o GCP gerencia o ambiente de execu√ß√£o, atualiza√ß√µes e patches de seguran√ßa.

Em conjunto, essas tecnologias formam uma arquitetura moderna e eficiente, que capitaliza os pontos fortes de cada servi√ßo GCP para construir um pipeline de dados robusto, escal√°vel e de baixo custo, capaz de suportar as demandas atuais e futuras de an√°lise de dados de ENA e integra√ß√£o com solu√ß√µes de IA.




## 5. Identifica√ß√£o de Melhorias e Boas Pr√°ticas

Com base na an√°lise detalhada dos componentes do pipeline de dados ENA, diversas oportunidades de melhoria e a ado√ß√£o de boas pr√°ticas podem ser identificadas para aprimorar ainda mais a robustez, a efici√™ncia, a seguran√ßa e a manutenibilidade do projeto. Estas sugest√µes abrangem desde otimiza√ß√µes de c√≥digo at√© a implementa√ß√£o de mecanismos de governan√ßa e monitoramento.

### 5.1. Melhorias para a Tabela Externa Raw

A camada RAW, embora funcional, pode ser otimizada para melhor desempenho e governan√ßa. A principal recomenda√ß√£o √© a ado√ß√£o do particionamento por Hive e a imposi√ß√£o de filtros de parti√ß√£o obrigat√≥rios. A inclus√£o da op√ß√£o `hive_partitioning_mode = 'AUTO'` nas `OPTIONS` da tabela externa permitiria que o BigQuery inferisse automaticamente as parti√ß√µes baseadas na estrutura de diret√≥rios do GCS (e.g., `dt=YYYY-MM-DD`). Isso otimizaria significativamente as consultas que filtram por data, pois o BigQuery seria capaz de pular diret√≥rios n√£o relevantes, reduzindo a quantidade de dados escaneados e, consequentemente, os custos e o tempo de execu√ß√£o. Atualmente, o filtro √© feito via `_FILE_NAME` na `MERGE` statement, o que √© funcional, mas a infer√™ncia de parti√ß√£o pelo BigQuery √© inerentemente mais eficiente. Adicionalmente, a op√ß√£o `require_partition_filter = TRUE` nas `OPTIONS` for√ßaria os usu√°rios a incluir uma cl√°usula `WHERE` que filtre por parti√ß√£o em suas consultas √† tabela externa. Esta √© uma medida de governan√ßa de dados que ajuda a evitar varreduras completas da tabela (full table scans) acidentais, que podem ser custosas em grandes volumes de dados. Embora o procedimento armazenado j√° filtre por `_FILE_NAME`, essa op√ß√£o adicionaria uma camada de seguran√ßa para consultas ad-hoc. Para garantir a consist√™ncia dos dados, especialmente em um ambiente onde a fonte de dados pode variar, a implementa√ß√£o de valida√ß√µes de schema no GCS (por exemplo, usando ferramentas como o Apache Avro ou Parquet com schema definido) antes da ingest√£o poderia ser ben√©fica. Embora o BigQuery seja tolerante a schemas, a valida√ß√£o antecipada pode prevenir erros a jusante no pipeline.

### 5.2. Melhorias para o Procedimento Armazenado do BigQuery

O procedimento armazenado √© o motor de transforma√ß√£o e, como tal, se beneficia de melhorias em observabilidade e resili√™ncia. A implementa√ß√£o de um mecanismo de logging mais detalhado √© fundamental. Isso pode ser feito inserindo registros em uma tabela de log dedicada que capture informa√ß√µes como o status da execu√ß√£o (sucesso/falha), a data de processamento, o n√∫mero de registros inseridos/atualizados e mensagens de erro. Este logging √© crucial para depura√ß√£o e monitoramento da sa√∫de do pipeline em produ√ß√£o. Al√©m disso, adicionar valida√ß√µes expl√≠citas para cen√°rios espec√≠ficos, como verificar se a parti√ß√£o de dados para `p_data_processamento` existe na camada RAW antes de tentar process√°-la, aumentaria a robustez. Se a parti√ß√£o estiver vazia ou n√£o existir, o procedimento poderia registrar um aviso ou erro e sair, em vez de tentar processar dados inexistentes. A instru√ß√£o `IF COUNT(*) = 0 THEN RAISE ERROR` pode ser usada para interromper a execu√ß√£o em caso de condi√ß√µes inesperadas. Para procedimentos mais complexos, considerar a cria√ß√£o de fun√ß√µes auxiliares ou sub-procedimentos para encapsular l√≥gicas espec√≠ficas, melhorando a legibilidade e a manutenibilidade do c√≥digo SQL.

### 5.3. Melhorias para a An√°lise Preliminar de Dados

A an√°lise preliminar de dados √© uma etapa cr√≠tica para a garantia de qualidade e pode ser aprimorada atrav√©s de automa√ß√£o e integra√ß√£o com ferramentas de governan√ßa. As queries de an√°lise preliminar podem ser configuradas como *Scheduled Queries* no BigQuery, permitindo a execu√ß√£o autom√°tica di√°ria ou em outra frequ√™ncia definida, gerando relat√≥rios de qualidade de dados de forma proativa. Os resultados poderiam ser armazenados em uma tabela de auditoria ou enviados para um sistema de alerta. Para um pipeline de dados mais maduro, a integra√ß√£o com ferramentas como Dataform ou dbt (data build tool) seria altamente ben√©fica, permitindo versionar o c√≥digo SQL, criar testes de qualidade de dados como parte do pipeline de transforma√ß√£o e documentar o *lineage* dos dados. Os resultados das queries de qualidade de dados podem ser facilmente conectados a ferramentas de visualiza√ß√£o como Looker ou Data Studio para a cria√ß√£o de dashboards interativos, permitindo que analistas e stakeholders monitorem a qualidade dos dados em tempo real. Para cada m√©trica de qualidade de dados, poderiam ser definidos limiares, e se uma m√©trica exceder um limiar pr√©-definido, um alerta autom√°tico (via Cloud Monitoring, Pub/Sub, ou e-mail) poderia ser disparado. Para queries de qualidade de dados que s√£o executadas frequentemente ou que envolvem grandes volumes de dados, a cria√ß√£o de *views materializadas* no BigQuery pode otimizar o desempenho.

### 5.4. Melhorias para a Tabela Trusted

A tabela TRUSTED √© a base para o consumo de dados e pode ser otimizada para desempenho e governan√ßa. A adi√ß√£o de `CLUSTER BY cod_resplanejamento, ena_data` pode refinar ainda mais a organiza√ß√£o f√≠sica dos dados dentro de cada parti√ß√£o, acelerando consultas que filtram ou agregam por essas colunas. Embora o BigQuery n√£o imponha chaves prim√°rias, √© uma boa pr√°tica documentar conceitualmente quais colunas formam a chave prim√°ria e garantir sua unicidade atrav√©s da l√≥gica do procedimento `MERGE`. Para consultas de alto desempenho que s√£o executadas com muita frequ√™ncia e envolvem agrega√ß√µes complexas, a cria√ß√£o de *views materializadas* pode ser uma solu√ß√£o eficaz. Para gerenciar o custo de armazenamento e a conformidade com pol√≠ticas de reten√ß√£o de dados, configurar pol√≠ticas de expira√ß√£o de dados para a tabela ou para parti√ß√µes espec√≠ficas. Refor√ßar o controle de acesso (IAM) na tabela `trusted`, concedendo permiss√µes de leitura apenas para usu√°rios e servi√ßos que realmente precisam acessar esses dados, e permiss√µes de escrita apenas para o servi√ßo que executa o procedimento de atualiza√ß√£o.

### 5.5. Melhorias para a Cloud Function

A Cloud Function, como orquestrador, pode ser aprimorada em termos de seguran√ßa, resili√™ncia e monitoramento. A implementa√ß√£o de mecanismos de autentica√ß√£o e autoriza√ß√£o √© crucial, como restringir o acesso via Cloud IAM, colocar um API Gateway na frente da fun√ß√£o, ou usar tokens de ID para invoca√ß√µes internas. Para procedimentos BigQuery que podem levar um tempo consider√°vel, uma melhoria seria iniciar o job do BigQuery de forma ass√≠ncrona e retornar imediatamente um `job_id` para o cliente, liberando a Cloud Function rapidamente. Al√©m do logging b√°sico, integrar a fun√ß√£o com o Cloud Monitoring para criar m√©tricas personalizadas e configurar alertas baseados nessas m√©tricas. Evitar o *hardcoding* de valores como `project_dataset_procedure` utilizando vari√°veis de ambiente, tornando a fun√ß√£o mais flex√≠vel e segura. Desenvolver testes unit√°rios e de integra√ß√£o para a l√≥gica da Cloud Function, utilizando *mocks* para simular as intera√ß√µes com o cliente BigQuery. Para automa√ß√£o di√°ria, configurar um job no Cloud Scheduler que invoque a Cloud Function via HTTP. Por fim, uma valida√ß√£o mais rigorosa do formato da data (se fornecida) poderia ser implementada para evitar que datas mal formatadas cheguem ao BigQuery e causem erros no procedimento armazenado.


# üìä Projeto: Sauter University 2025 Challenge
**Autor:** *Marcos Ven√≠cio Silva do Nascimento*  
**Fun√ß√£o:** Analista de Dados  

---

## üìå Vis√£o Geral do Projeto
O projeto foi desenvolvido por uma equipe multidisciplinar composta por **Engenheiros de Dados, DevOps, Infraestrutura e Machine Learning**, cobrindo toda a pipeline de dados.  

O objetivo central foi **tratar, analisar e disponibilizar dados do ONS (Operador Nacional do Sistema El√©trico)** sobre a **Energia Natural Afluente (ENA) em reservat√≥rios hidrel√©tricos**, permitindo **an√°lises estat√≠sticas, preditivas e a constru√ß√£o de dashboards interativos** para suporte √† tomada de decis√£o.  

Como Analista de Dados, minha atua√ß√£o concentrou-se em:
- **Explora√ß√£o e limpeza dos dados (Data Wrangling).**
- **An√°lise estat√≠stica e visualiza√ß√£o de tend√™ncias.**
- **Tratamento de valores nulos e padroniza√ß√£o de colunas.**
- **Integra√ß√£o e unifica√ß√£o de m√∫ltiplos arquivos CSV (2020‚Äì2025).**
- **Constru√ß√£o de dashboards interativos no Looker Studio.**

Ambiente de desenvolvimento:
- **Google Cloud Platform (GCP)**  
- **Google Colab** para processamento e an√°lise explorat√≥ria.  
- **Looker Studio** para visualiza√ß√£o e dashboards.  

---

## üîÑ Pipeline de Dados
1. **Ingest√£o**: Arquivos CSV anuais (2020‚Äì2025) foram importados e unificados.  
2. **Tratamento**:  
   - Convers√£o de datas para formato `datetime`.  
   - Padroniza√ß√£o dos nomes de colunas (`snake_case`).  
   - Preenchimento de valores nulos com **mediana (num√©ricos)** e **moda (categ√≥ricos)**.  
   - Remo√ß√£o de colunas redundantes.  
3. **An√°lise Explorat√≥ria**:  
   - Tend√™ncias mensais por subsistema.  
   - Sazonalidade da ENA bruta e armazen√°vel.  
   - M√°ximos anuais por bacia hidrogr√°fica.  
   - Distribui√ß√£o estat√≠stica por boxplots.  
4. **Exporta√ß√£o**: Dados limpos foram salvos em `.csv` e `.xlsx` para integra√ß√£o no pipeline confi√°vel.  
5. **Visualiza√ß√£o**: Cria√ß√£o de dashboards interativos no **Looker Studio**.  

---

## üßë‚Äçüíª Principais Contribui√ß√µes (com c√≥digo documentado)

### 1. Unifica√ß√£o dos Arquivos (2020‚Äì2025)
```python
dfs = [pd.read_csv(arq, sep=";", encoding="utf-8") for arq in arquivos]
df = pd.concat(dfs, ignore_index=True)
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
df["ena_data"] = pd.to_datetime(df["ena_data"], errors="coerce")
df.to_csv("ENA_DIARIO_RESERVATORIOS_2020_2025.csv", sep=";", encoding="utf-8", index=False)
```

### 2. Tratamento de Valores Nulos
```python
numeric_cols_with_nulls = df.select_dtypes(include=['number']).columns[df.isnull().any()]
for col in numeric_cols_with_nulls:
    median_val = df[col].median()
    df[col] = df[col].fillna(median_val)

mode_nom_ree = df['nom_ree'].mode()[0]
df['nom_ree'] = df['nom_ree'].fillna(mode_nom_ree)
```

### 3. Tend√™ncia Mensal por Subsistema
```python
df["mes_ano"] = df["ena_data"].dt.to_period("M")
monthly_ena = df.groupby(["mes_ano", "nom_subsistema"])["ena_bruta_res_mwmed"].mean().reset_index()
monthly_ena["mes_ano"] = monthly_ena["mes_ano"].dt.to_timestamp()

sns.lineplot(data=monthly_ena, x="mes_ano", y="ena_bruta_res_mwmed", hue="nom_subsistema")
```

### 4. M√°ximos Anuais por Bacia Hidrogr√°fica
```python
df['ano'] = df['ena_data'].dt.year
max_ena_anual_por_bacia = df.groupby(['nom_bacia', 'ano'])[['ena_bruta_res_mwmed','ena_armazenavel_res_mwmed']].max()
mean_max_ena_por_bacia = max_ena_anual_por_bacia.groupby('nom_bacia').mean().reset_index()
```

### 5. Exporta√ß√£o para Integra√ß√£o
```python
df.to_csv('ena_trusted_cleaned_final.csv', index=False)
```

---

## üìà Dashboard no Looker Studio
- Visualiza√ß√µes desenvolvidas:  
  - **Evolu√ß√£o temporal da ENA Bruta e Armazen√°vel por Bacia.**  
  - **Compara√ß√£o entre subsistemas.**  
  - **Sazonalidade m√©dia anual.**  
  - **Distribui√ß√£o estat√≠stica (boxplots).**  
- Acesso: *(inserir link se aplic√°vel)*  

---

## ‚úÖ Resultados Obtidos
- Dataset confi√°vel e unificado para todo o per√≠odo **2020‚Äì2025**.  
- Redu√ß√£o de inconsist√™ncias via tratamento de nulos.  
- Gera√ß√£o de insights sobre **sazonalidade, m√°ximos anuais e varia√ß√µes por subsistema**.  
- Dashboard interativo permitindo an√°lise din√¢mica por gestores e pesquisadores.  

---

## üöÄ Conclus√£o
Minha participa√ß√£o como **Analista de Dados** foi essencial para **transformar dados brutos do ONS em informa√ß√£o estrat√©gica**, integrando a pipeline de ponta a ponta, desde ingest√£o at√© visualiza√ß√£o no Looker Studio.  
---



# Documenta√ß√£o do Projeto: Pipeline de treinamento 

# üìä Projeto: Sauter University 2025 Challenge

  

**Autor:** St√™nio Medeiros Freitas  
**Fun√ß√£o:** Engenharia Machine Learning  

---

## 1. Introdu√ß√£o

O Jupyter Notebook implementa um pipeline completo de Machine Learning (ML) e An√°lise de S√©ries Temporais para prever o Percentual de Energia Armazenada em Reservat√≥rios (EARpercentual).  

O projeto utiliza dados hist√≥ricos di√°rios fornecidos pelo **Operador Nacional do Sistema El√©trico (ONS)**, cobrindo o per√≠odo de **2000 a 2025**.  

O fluxo de trabalho √© estruturado em tr√™s macroetapas:  

1. Coleta e Pr√©-Processamento  
2. An√°lise de S√©ries Temporais Estat√≠sticas  
3. Modelagem Preditiva (LightGBM)  

---

## 2. Coleta e Pr√©-Processamento de Dados

### 2.1. Aquisi√ß√£o e Consolida√ß√£o de Dados

O Notebook inicia a fase de coleta de dados da ONS para cada ano (2000 a 2025):  

- **Energia Natural Afluente (ENA) Di√°ria por Bacia**: Arquivos CSV, que fornecem informa√ß√µes sobre o fluxo de energia (vaz√£o) que chega √†s bacias hidrogr√°ficas.  
- **Energia Armazenada (EAR) Di√°ria por Reservat√≥rio**: Arquivos Parquet, contendo os n√≠veis de armazenamento espec√≠ficos de cada reservat√≥rio.  

Os arquivos baixados s√£o validados quanto ao tamanho (evitando arquivos vazios) e concatenados em dois DataFrames principais: `df_ENA` e `df_EAR`.

---

### 2.2. Limpeza, Tipagem e Jun√ß√£o de Dados

Etapas realizadas:  

- **Convers√£o de Datas:** As colunas `ena_data` e `ear_data` s√£o convertidas para o formato `datetime`.  
- **Tipagem Num√©rica:** Colunas num√©ricas em `df_EAR` (como `ear_reservatorio_subsistema_*` e `val_contribear_*`) s√£o convertidas para *float*, utilizando `errors='coerce'` para tratar valores n√£o num√©ricos como `NaN`.  
- **Merge:** Os dois DataFrames s√£o mesclados (`pd.merge`) em uma jun√ß√£o interna (`how='inner'`), utilizando a data e o nome da bacia (`nom_bacia`) como chaves, resultando no DataFrame `df_merged`.  
- **Ajuste Final:** Colunas redundantes (`data_ear`, `cod_resplanejamento`) s√£o removidas, e a coluna de data √© definida como o √≠ndice principal do DataFrame.  

---

### 2.3. Agrega√ß√£o Temporal para An√°lise (Mensal)

Para an√°lise com modelos estat√≠sticos (ARIMA/SARIMAX), os dados di√°rios s√£o convertidos para frequ√™ncia mensal:  

- **Valores Num√©ricos:** Calculada a m√©dia mensal.  
- **Vari√°veis Categ√≥ricas:** Preservado o primeiro valor do m√™s.  

---

## 3. An√°lise de S√©ries Temporais (Modelos Estat√≠sticos)

### 3.1. Estacionariedade e Decomposi√ß√£o

- **Filtragem de S√©ries:** Apenas s√©ries temporais sem valores ausentes (`NaN`) s√£o mantidas.  
- **Visualiza√ß√£o de Padr√µes:** Gr√°ficos de M√©dia M√≥vel e Desvio Padr√£o M√≥vel verificam visualmente a estacionariedade.  
- **Decomposi√ß√£o Sazonal:** Aplica√ß√£o da fun√ß√£o `seasonal_decompose` (modelo aditivo, per√≠odo = 12) para extrair **Tend√™ncia, Sazonalidade e Res√≠duo**.  
- **ACF e PACF:** Gr√°ficos de Autocorrela√ß√£o e Autocorrela√ß√£o Parcial para defini√ß√£o emp√≠rica de par√¢metros `p` e `q`.  

---

### 3.2. Determina√ß√£o da Diferencia√ß√£o (d)

O Teste Aumentado de Dickey-Fuller (ADF) √© utilizado para determinar o n√∫mero de diferencia√ß√µes necess√°rias (`d = 0, 1 ou 2`).  

---

### 3.3. Modelagem ARIMA e SARIMA

- **Divis√£o de Dados:** Treino (70%), Valida√ß√£o (10%), Teste (20%).  
- **Ajuste:**  
  - ARIMA: diferencia√ß√£o manual aplicada antes do ajuste, previs√µes reintegradas para a escala original.  
  - SARIMA: tratamento interno da diferencia√ß√£o e ordens sazonais (`P,D,Q,s`) com `s=12` (sazonalidade anual).  
- **M√©tricas:** Avalia√ß√£o com **MSE, RMSE, R¬≤ e MAPE**.  
- **Sa√≠da:** Arquivos CSV com previs√µes de teste para cada reservat√≥rio.  

---

## 4. Modelagem Preditiva com LightGBM

### 4.1. Engenharia Avan√ßada de Features (Di√°ria)

- **Features Temporais:** Ano, m√™s, dia, dia da semana, trimestre + representa√ß√µes c√≠clicas (seno/cosseno).  
- **Lags e Rolamentos:** Defasagens de 1, 7 e 30 dias + estat√≠sticas m√≥veis (m√©dia e desvio padr√£o em janelas de 7 e 30 dias).  
- **Codifica√ß√£o Categ√≥rica:** Uso de *Target Encoder* para vari√°veis de alta cardinalidade (`nom_reservatorio`).  
- **Features Agregadas:** Vari√°veis com m√©dia do target por bacia ou subsistema no mesmo dia.  

---

### 4.2. Treinamento e Avalia√ß√£o do LightGBM

- **Modelo:** `LGBMRegressor`, escolhido pela efici√™ncia e suporte a colunas categ√≥ricas.  
- **Valida√ß√£o Temporal:** `TimeSeriesSplit` com 5 folds sequenciais.  
- **Import√¢ncia de Features:** Avalia√ß√£o de import√¢ncia pelo crit√©rio *gain*.  

---

### 4.3. Exporta√ß√£o e Resultados Finais

- Modelo final exportado como `lgbm_model.pkl`.  
- Gera√ß√£o do arquivo CSV: `previsoes_teste_todos_reservatorios_todos_anos.csv`.  
- Colunas do CSV: `data, nom_reservatorio, valor_real, valor_previsto, erro_absoluto, erro_percentual`.  
- Visualiza√ß√£o: gr√°fico de desempenho em um reservat√≥rio de exemplo.  

---


